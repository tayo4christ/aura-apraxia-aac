# üß† AURA Research Notes

## 1. Background

**AURA (Adaptive Understanding and Relearning Assistant)** extends the ideas introduced in the position paper
**‚ÄúGesture as Code: Rethinking Programming Through Multimodal Interaction with AI for Apraxia Therapy,‚Äù**
presented at the **Aarhus 2025 Workshop ‚Äî *The End of Programming as We Know It: Envisioning Radical
Re-Conceptualizations of Co-Coding with AI*.**

The workshop paper proposed a speculative re-conceptualization of programming as an **embodied, dialogic, and
adaptive process** rather than a purely symbolic one. AURA demonstrates this theory in practice: instead of writing
code, users interact through **speech, gesture, and adaptive feedback**, co-creating meaning with the AI system.
This reframing positions AI as a **co-author of interaction**, supporting individuals with **Apraxia of Speech (AOS)**
and related motor-speech disorders.

---

## 2. Connection Between AURA and the Workshop Paper

| Concept from Workshop | Implementation in AURA |
|------------------------|------------------------|
| **Programming as Co-Adaptation** ‚Äì AI and user negotiate meaning through partial inputs | Reinforcement learning engine that adapts therapy tasks dynamically |
| **Programming Without Syntax** ‚Äì interaction via gesture and speech instead of text | Gesture-to-Speech AAC module using Mediapipe and TTS |
| **Programming as Dialogue** ‚Äì multimodal, feedback-driven exchange | Real-time feedback loops integrating speech recognition (Wav2Vec2) and error classification (CNN-BiLSTM) |
| **Inclusive, Embodied Computation** ‚Äì enabling non-verbal or neurodivergent users to engage with AI | Adaptive therapy framework designed for users with Apraxia of Speech |

AURA therefore operationalizes the *Gesture as Code* vision by offering a **functional prototype** that makes
co-programming with AI tangible within therapeutic and educational contexts.

---

## 3. Future Research Directions

- **Transformer-Based Gesture Recognition:** integrate Vision Transformers for more nuanced multimodal gesture analysis.
- **Cross-Modal Learning:** explore shared embeddings between gesture and speech for unified representation.
- **Edge AI Deployment:** optimize models for low-power devices to support in-home therapy sessions.
- **Personalized Reinforcement Learning:** tailor therapy to user progress and emotional feedback.
- **Ethical and Accessibility Frameworks:** evaluate data privacy, consent, and inclusivity in AI-mediated therapy.

---

## 4. Potential Collaborations

- **Speech and Language Therapy Researchers:** for clinical validation and therapy dataset co-creation.
- **HCI and Accessibility Labs:** to study embodied interaction and inclusive design patterns.
- **AI and Multimodal Interaction Groups:** for exploring new co-coding paradigms in health and education.
- **Open-Source Accessibility Communities:** to broaden deployment and feedback from assistive-tech developers.

---

### Citation

Omoyemi, O. (2025). *Gesture as Code: Rethinking Programming Through Multimodal Interaction with AI for Apraxia Therapy.*
Workshop on **The End of Programming as We Know It ‚Äî Envisioning Radical Re-Conceptualizations of Co-Coding with AI**,
Aarhus 2025 Conference (Denmark).
[Workshop Submissions](https://glossy-twister-5ec.notion.site/Workshop-submissions-1c7e06f235ea8019ab1fd8cea7151742)

---
